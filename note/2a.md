# Part 2A

在 part 2a 中，其实是实现了 **Raft 算法的逻辑**。
整个过程包括了以下几个部分：

- Leader election
- Log replication
- Raw node interface

## Leader election

其中在 Leader election 中，我们其实需要面对的是一个 raft 集群在各种状态下的 leader 选举。

在 TinyKV 中，使用的是逻辑时钟，由代码进行控制以进行测试。

会有不同的条件将各个节点转变为不同的角色。
其中，若符合条件，就会用以下的一些函数来控制

```go
r.becomeFollower()
r.becomeCandidate()
r.becomeLeader()
```

所谓的不同条件，就是该节点接收各种不同的消息，做出不同的反应。在此无法全面概述 raft 算法该怎么办，举一些例子：

比如我们需要对 Term 进行识别处理：

1. 当 incoming message 的 Term 大于本地 Term 时且消息格式是`msg_append`、`msg_append`等时，显然需要将当前的角色(role)使用`r.becomeFollower()`修改为 Folloer。

若通过 Term 处理之后还需要进一步处理，我们需要对具体的消息格式作出不同的处理：

2. 当 MsgType 为 `Msg_Hup` （该消息类型为 tinykv 的 raft 算法中内置类型，用于发送给自身节点告知自身是时候参加选举了）时，需要使当前节点开始进行选举流程。（使用的是`r.campaign()`函数）

进行选举时，节点角色会通过`r.becomeCandidate()`转变为 candidate，并进行一系列操作。比如发送请求投票的信息（MsgRequestVote），接受投票响应（MsgRequestVoteResponse）。
不管是何种消息，都需要进行相应的逻辑处理。当然，如果响应投票成功数量超过集群数量的一般，就可以不再处理该消息了，直接转变成 Leader。

3. Leader 选举部分其实也和日志有关系，因为如果我们要真正地判断一个节点是不是“更新”的节点，我们不仅需要看 Term，当 Term 一致的情况下，我们会比较各自的日志（在消息中会携带日志条目的 Term，index）等信息。一般来说，logterm 更大的则更新，若一致则 lastindex 更大或相等的，就可以接受其是“更新的节点”的事实。

4. 等等...

## Log replication

日志复制这个部分最为复杂，其中还关系到我们 Part1 的一些函数有没有编写正确。因为日志复制涉及到各种情况，比如一些节点只同步了部分日志就 down 掉了，重启之后该怎么办？Leader 突然 down 掉了，集群中又有了新的 Leader，重启之后日志怎么办？这些问题都十分复杂，重要的就是编写好正确的逻辑，当逻辑正确之后，我们就不需要在意情况到底会怎么改变，因为都是按正确的处理方式在走，终归是正确的。

几个关键：
- applied <= committed <= stabled
- 所有的 unstableentries 就是 raftLog 结构中，l.stabled - l.entries[0].Index + 1（含） 之后的所有 entries。因为，stabled 记录的是 index，整个结构中 index 是每个 entries 的标识，stabled 之后的就是 unstable，stabled 就认为是不会变的，applied 之后的是应用到整个 raft 集群中的。
- 一些需要通过test的细节问题。
- prog、next等注意如何更新。



对于原生的 unstable 结构中，前半部分是快照数据，而后半部分是日志条目组成的数组 entries，另外 unstable.offset 成员保存的是 entries 数组中的第一条数据在 raft 日志中的索引，即第 i 条 entries 数组数据在 raft 日志中的索引为 i + unstable.offset。

这两个部分，并不同时存在，同一时间只有一个部分存在。其中，快照数据仅当当前节点在接收从 leader 发送过来的快照数据时存在，在接收快照数据的时候，entries 数组中是没有数据的；除了这种情况之外，就只会存在 entries 数组的数据了。因此，当接收完毕快照数据进入正常的接收日志流程时，快照数据将被置空。

理解了以上 unstable 中数据的分布情况，就不难理解 unstable 各个函数成员的作用了，下面逐一进行解释。

每次当选 leader 之后，要追加一条 no-op 操作，原因是在某些情况下，一条日志同步到了大多数节点但还没得及 commit 就崩溃了。这样可能会使得一条已经同步到大多数节点的日志被 term 更小的日志覆盖从而使得其丢失。
每次追加 no-op 日志并同步，不去单独同步每一个往期日志。
